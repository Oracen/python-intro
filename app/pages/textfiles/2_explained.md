The most important similarity is that we're looking for points that are close to each other in some sense. The most important difference is how we measure what we mean by similar. In NLP, we like to think of it as learning something about the semantics of words with relation to the task. There definitely seem to be cases that suggest this is true - at least some of the time. With our compressor model however, we instead concentrate purely on how much overlap there is between our two inputs. This is purely information theoretic, no learning whatsoever.

One thing we can immediately notice is the two approaches are both doing the same thing - compression. "Old hands" won't be surprised by this conclusion, but newcomers may be confused into thinking the machine is literally learning. With enough time, it becomes pretty clear to most practitioners that this GZip model is just a distillation of what our models are doing during the training process. Really, our models are just doing a slightly more sophisticated (and nuanced) form of data compression.

A second point emerges when we consider that the GZip algorithm is purely looking at occurrences of word- and sub-word combinations. This is similar to older NLP methods which use n-grams and word counts as input features, but using shared information to select which elements are relevant to a task and to normalise for sequence length. Using information directly without involving parameters means this method is relatively data efficient. It's also worth noting that byte-pair encoding has become increasingly common for large language models.

Contrast this zero-training, data efficient approach to the typical NLP method. Normally, when we have a text classification problem we reach straight for Huggingface and do some transfer learning. After all, "all NLP problems are hard" and that justifies jumping up in complexity. Even though transfer learning improves our data efficiency, we still require a lot of data to get even moderately successful results.

Finally, there's a lesson in humility. We often motivate our explanation of these models through the lens of black boxes and learning latent meanings, rather than information theory and basic encoding. The fact that compression-based techniques perform well on a number of tasks thought to be difficult suggests the tasks may not be as hard as we expected. It definitely calls into question how we interpret what a lot of our heavyweight models are actually doing, and if the only result of this paper is less wooly thinking about ML, our field will benefit. At the very least, we should think twice before reaching for the largest, coolest, most complicated SOTA model.

Now, nobody is saying this will replace ChatGPT anytime soon, but the fact that this can do so well on benchmarks is suggestive. It confirms certain feature engineering practices (BPE, n-gram and lemmatisation) even deep into the era of transformers. It also suggests that the issue with NLP tasks may not be the inherent difficulty of language, but rather one of finding an efficient representation. It also reminds us that our models do have interpretable analogues in traditional computing, and that we should be hasty in jumping to intuitive-but-complex answers. This GZip model is a handy reminder that Occam's Razor and the KISS principle apply to the study of artificial intelligence, too.
